# Awesome-PEFT
### Papers
|Paper Title|Proposed Method|Venue and Year|Materials|
|------------------------------|------------------------------|------------------------------|------------------------------|
|Parameter-Efficient Transfer Learning for NLP|Adapter tuning|ICML PMLR 2019|[[PUB](https://proceedings.mlr.press/v97/houlsby19a.html)][[CODE](https://github.com/google-research/adapter-bert)]|
|Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning|VLM|EMNLP 2020|[[PUB](https://aclanthology.org/2020.findings-emnlp.41/)]|
|AdapterFusion: Non-destructive task composition for transfer learning|AdapterFusion|EACL 2021|[[PUB](https://aclanthology.org/2021.eacl-main.39/?utm_campaign=NLP%20News&utm_medium=email&utm_source=Revue%20newsletter)]|
|AdapterDrop: On the efficiency of adapters in transformers|AdapterDrop|EMNLP 2021|[[PUB](https://aclanthology.org/2021.emnlp-main.626/)]|
|Prefix-Tuning: Optimizing Continuous Prompts for Generation|Prefix tuning|ACL 2021|[[PDF](https://xiangli1999.github.io/pdf/prefix_tuning.pdf)][[CODE](https://github.com/XiangLi1999/PrefixTuning)]|
|The Power of Scale for Parameter-Efficient Prompt Tuning| Prompt tuning|EMNLP 2021|[[PUB](https://aclanthology.org/2021.emnlp-main.243/)]|
|Towards a Unified View of Parameter-Efficient Transfer Learning|MAM Adapter|ICLR 2022|[[PDF](https://xuezhemax.github.io/assets/publications/pdfs/iclr2022_towards.pdf)][[CODE](https://github.com/jxhe/unify-parameter-efficient-tuning)]|
|UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning|UniPELT|ACL 2022|[[PUB](https://aclanthology.org/2022.acl-long.433/)]|
|P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks|P-tuning v2|ACL 2022|[[PUB](https://aclanthology.org/2022.acl-short.8/)]|
|BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models|BitFit|ACL 2022|[[PUB](https://aclanthology.org/2022.acl-short.1/)]|
|LoRA: Low-Rank Adaptation of Large Language Models|LoRA|ICLR 2022|[[PDF](https://openreview.net/pdf?id=nZeVKeeFYf9)][[CODE](https://github.com/microsoft/LoRA)]|
|Adaptive budget allocation for parameter-efficient fine-tuning|AdaLoRA|ICLR 2023|[[PDF](https://par.nsf.gov/servlets/purl/10471451)][[CODE](https://github.com/QingruZhang/AdaLoRA)]|
|GPT Understands, Too|P-tuning|AI Open 2024|[[PUB](https://www.sciencedirect.com/science/article/pii/S2666651023000141)]|
|LoRA+: Efficient Low Rank Adaptation of Large Models|LoRA+|ICML PMLR 2024|[[PDF](https://openreview.net/pdf?id=NEv8YqBROO)][[CODE](https://github.com/nikhil-ghosh-berkeley/loraplus)]|
|Vector-based Random Matrix Adaptation|VeRA|ICLR 2024|[[PUB](https://dkopi.github.io/vera/)]|
|DoRA: Weight-Decomposed Low-Rank Adaptation|DoRA|ICML PMLR 2024|[[PDF](https://openreview.net/pdf?id=3d5CIRG1n2)][[CODE](https://github.com/NVlabs/DoRA)]|
### Libraries
