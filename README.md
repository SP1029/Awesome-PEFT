# Awesome-PEFT
### Papers
|Paper Title|Proposed Method|Venue and Year|Materials|
|------------------------------|------------------------------|------------------------------|------------------------------|
|Parameter-Efficient Transfer Learning for NLP|Adapter tuning|ICML PMLR 2019|[[PUB](https://proceedings.mlr.press/v97/houlsby19a.html)][[CODE](https://github.com/google-research/adapter-bert)]|
|Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning|VLM|EMNLP 2020|[[PUB](https://aclanthology.org/2020.findings-emnlp.41/)]|
|AdapterFusion: Non-destructive task composition for transfer learning|AdapterFusion|EACL 2021|[[PUB](https://aclanthology.org/2021.eacl-main.39/?utm_campaign=NLP%20News&utm_medium=email&utm_source=Revue%20newsletter)]|
|AdapterDrop: On the efficiency of adapters in transformers|AdapterDrop|EMNLP 2021|[[PUB](https://aclanthology.org/2021.emnlp-main.626/)]|
|Prefix-Tuning: Optimizing Continuous Prompts for Generation|Prefix tuning|ACL 2021|[[PDF](https://xiangli1999.github.io/pdf/prefix_tuning.pdf)][[CODE](https://github.com/XiangLi1999/PrefixTuning)]|
|The Power of Scale for Parameter-Efficient Prompt Tuning| Prompt tuning|EMNLP 2021|[[PUB](https://aclanthology.org/2021.emnlp-main.243/)]|
|Parameter-Efficient Transfer Learning with Diff Pruning|DiffPruning|ACL 2021|[[PUB](https://aclanthology.org/2021.acl-long.378/)]|
|Compacter: Efficient low-rank hypercomplex adapter layers|Compacter|NeurIPS 2021|[[PDF](https://proceedings.neurips.cc/paper/2021/file/081be9fdff07f3bc808f935906ef70c0-Paper.pdf)][[CODE](https://github.com/rabeehk/compacter)]|
|Towards a Unified View of Parameter-Efficient Transfer Learning|MAM Adapter, ParallelAdapter|ICLR 2022|[[PDF](https://xuezhemax.github.io/assets/publications/pdfs/iclr2022_towards.pdf)][[CODE](https://github.com/jxhe/unify-parameter-efficient-tuning)]|
|UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning|UniPELT|ACL 2022|[[PUB](https://aclanthology.org/2022.acl-long.433/)]|
|P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks|P-tuning v2|ACL 2022|[[PUB](https://aclanthology.org/2022.acl-short.8/)]|
|BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models|BitFit|ACL 2022|[[PUB](https://aclanthology.org/2022.acl-short.1/)]|
|LoRA: Low-Rank Adaptation of Large Language Models|LoRA|ICLR 2022|[[PDF](https://openreview.net/pdf?id=nZeVKeeFYf9)][[CODE](https://github.com/microsoft/LoRA)]|
|Efficient Fine-Tuning of BERT Models on the Edge|FAR|ISCAS 2022|[[PUB](https://ieeexplore.ieee.org/abstract/document/9937567)][[PDF](https://arxiv.org/pdf/2205.01541)]|
|SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters|SparseAdapter|EMNLP 2022|[[PUB](https://aclanthology.org/2022.findings-emnlp.160/)][[CODE](https://github.com/Shwai-He/SparseAdapter)]|
|Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning|(IA)<SUP>3<SUP/>|NeurIPS 2022|[[PUB](https://proceedings.neurips.cc/paper_files/paper/2022/hash/0cde695b83bd186c1fd456302888454c-Abstract-Conference.html)][[CODE](https://github.com/r-three/t-few)]|
| Adamix: Mixture-of adapter for parameter-efficient tuning of large language models|Adamix|-----|[[PDF](https://www.microsoft.com/en-us/research/uploads/prod/2022/05/Mixture_of_Adapters-628fa6a57efd3.pdf)][[CODE](https://github.com/microsoft/AdaMix)]|
|Adaptive budget allocation for parameter-efficient fine-tuning|AdaLoRA|ICLR 2023|[[PDF](https://par.nsf.gov/servlets/purl/10471451)][[CODE](https://github.com/QingruZhang/AdaLoRA)]|
|QLoRA: Efficient Finetuning of Quantized LLMs|QLoRA|NeurIPS 2023|[[PUB](https://proceedings.neurips.cc/paper_files/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html)][[CODE](https://github.com/artidoro/qlora)]|
|Krona: Parameter efficient tuning with kronecker adapter|KronA|NeurIPS 2023|[[PDF](https://neurips2023-enlsp.github.io/papers/paper_61.pdf)]|
|GPT Understands, Too|P-tuning|AI Open 2024|[[PUB](https://www.sciencedirect.com/science/article/pii/S2666651023000141)]|
|LoRA+: Efficient Low Rank Adaptation of Large Models|LoRA+|ICML PMLR 2024|[[PDF](https://openreview.net/pdf?id=NEv8YqBROO)][[CODE](https://github.com/nikhil-ghosh-berkeley/loraplus)]|
|Vector-based Random Matrix Adaptation|VeRA|ICLR 2024|[[PUB](https://dkopi.github.io/vera/)]|
|LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models|LoftQ|ICLR 2024|[[PDF](https://openreview.net/pdf?id=LzPWWPAdY4)][[CODE]](https://github.com/yxli2123/LoftQ)|
|DoRA: Weight-Decomposed Low-Rank Adaptation|DoRA|ICML PMLR 2024|[[PDF](https://openreview.net/pdf?id=3d5CIRG1n2)][[CODE](https://github.com/NVlabs/DoRA)]|
|RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation|RoSA|ICML PMLR 2024|[[PDF](https://openreview.net/pdf?id=FYvpxyS43U)][[CODE](https://github.com/IST-DASLab/RoSA)]|
|ReFT: Representation Finetuning for Language Models|ReFT|-----|[[PDF](https://arxiv.org/pdf/2404.03592)][[CODE](https://github.com/stanfordnlp/pyreft)]|
### Libraries
