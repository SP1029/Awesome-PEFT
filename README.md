# Awesome-PEFT
### Papers
|Paper Title|Method|Venue and Year|Materials|
|------------------------------|------------------------------|------------------------------|------------------------------|
|Parameter-Efficient Transfer Learning for NLP|Adapter tuning|ICML PMLR 2019|[[PUB](https://proceedings.mlr.press/v97/houlsby19a.html)][[CODE](https://github.com/google-research/adapter-bert)]|
|Prefix-Tuning: Optimizing Continuous Prompts for Generation|Prefix tuning|ACL 2021|[[PDF](https://xiangli1999.github.io/pdf/prefix_tuning.pdf)][[CODE](https://github.com/XiangLi1999/PrefixTuning)]|
|The Power of Scale for Parameter-Efficient Prompt Tuning| Prompt tuning|EMNLP 2021|[[PUB](https://aclanthology.org/2021.emnlp-main.243/)]|
|GPT Understands, Too|P-tuning|AI Open 2024|[[PUB](https://www.sciencedirect.com/science/article/pii/S2666651023000141)]|
|P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks|P-tuning v2|ACL 2022|[[PUB](https://aclanthology.org/2022.acl-short.8/)]|
|BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models|BitFit|ACL 2022|[[PUB](https://aclanthology.org/2022.acl-short.1/)]|
|LoRA: Low-Rank Adaptation of Large Language Models|LoRA|ICLR 2022|[[PDF](https://openreview.net/pdf?id=nZeVKeeFYf9)][[CODE](https://github.com/microsoft/LoRA)]|
